{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25982386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vanshaj/medusaenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtune\n",
    "from torchtune.models.llama3_1 import llama3_1_8b_medusa\n",
    "from accelerate import init_empty_weights\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import json\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "from functools import partial\n",
    "from torchtune.models.llama3._tokenizer import Llama3Tokenizer\n",
    "from torchtune.datasets._chat import chat_dataset\n",
    "import gc\n",
    "\n",
    "global tokenizer\n",
    "def load_data(dataset_dir, tokenizer_dir, bs = 1):\n",
    "    global tokenizer\n",
    "    tokenizer = Llama3Tokenizer(tokenizer_dir)   \n",
    "\n",
    "    dataset = chat_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        source=\"json\",\n",
    "        data_files=dataset_dir,\n",
    "        conversation_column=\"messages\",\n",
    "        conversation_style=\"openai\",\n",
    "        split = 'train[80%:90%]'\n",
    "    )\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        collate_fn=torchtune.data.padded_collate_sft\n",
    "    )\n",
    "    return dataloader\n",
    "    \n",
    "def load_model(checkpoint_dir):\n",
    "    if checkpoint_dir == None:\n",
    "        return\n",
    "    with init_empty_weights():\n",
    "        model = llama3_1_8b_medusa()\n",
    "    checkpoint_dir += 'model-00001-of-00001.bin'\n",
    "    checkpoint = torch.load(checkpoint_dir, map_location = device)\n",
    "    model.load_state_dict(checkpoint, assign = True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    model.setup_caches(batch_size=1, dtype=torch.float32, decoder_max_seq_len=max_cache_size)\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def format_input(input_tokens):\n",
    "    assistant_token = 78191\n",
    "    assistant_append_tokens =  torch.tensor([128007], device = device)\n",
    "    tensor_2d = input_tokens  # shape: [1, seq_len]\n",
    "    tensor_1d = tensor_2d[0]  # shape: [seq_len]\n",
    "    for i in range(len(tensor_1d)):\n",
    "        if int(tensor_1d[i]) == assistant_token:\n",
    "            break\n",
    "    part1 = torch.cat((tensor_1d[:i+1], assistant_append_tokens))\n",
    "    # Optional: keep them 2D\n",
    "    part1 = part1.unsqueeze(0)\n",
    "    return part1\n",
    "\n",
    "def decode(x):\n",
    "    return tokenizer.decode(x.tolist()[0], skip_special_tokens = False )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "tokenizer_dir = '/home/ubuntu/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model'\n",
    "checkpoint_dir = '/home/ubuntu/vanshaj/inf2-training/3rdparty/torchtune/medusa_checkpoints/epoch_0/'\n",
    "dataset_dir = '/home/ubuntu/vanshaj/justpi.jsonl'\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:1')\n",
    "max_cache_size = 4096\n",
    "dataloader = load_data(dataset_dir, tokenizer_dir, bs = 1)\n",
    "# checkpoint_dir = None\n",
    "model = load_model(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8242a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51, 4096])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "KV-caches are setup for inference mode, input positions must be provided!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33maccept_lengths: \u001b[39m\u001b[33m\"\u001b[39m, accept_lengths)\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataloader, model, tokens_to_generate)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mbreakpoint\u001b[39m()\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(causal_mask.shape)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shape: [(1+n), bs, seq, vocab_dim]\u001b[39;00m\n\u001b[32m     59\u001b[39m base_logits = output[\u001b[32m0\u001b[39m][:, -\u001b[32m1\u001b[39m] \u001b[38;5;66;03m# shape: [bs, vocab_dim]\u001b[39;00m\n\u001b[32m     60\u001b[39m pred = base_logits.argmax(dim = -\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# shape: [bs, 1]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vanshaj/medusaenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vanshaj/medusaenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vanshaj/torchtune/torchtune/modules/transformer.py:887\u001b[39m, in \u001b[36mMedusaTransformerDecoder.forward\u001b[39m\u001b[34m(self, tokens, mask, encoder_input, encoder_mask, input_pos)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    881\u001b[39m \u001b[33;03mForward pass that returns both main LM head output and Medusa head outputs.\u001b[39;00m\n\u001b[32m    882\u001b[39m \n\u001b[32m    883\u001b[39m \u001b[33;03mThe combined output is formatted as follows:\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m[*hidden, output,num_medusa_heads*medusa_logits]\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# Get the base output from parent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m combined_output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m combined_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vanshaj/torchtune/torchtune/modules/transformer.py:654\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, tokens, mask, encoder_input, encoder_mask, input_pos, input_embeds)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    587\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    588\u001b[39m     tokens: Optional[torch.Tensor],\n\u001b[32m   (...)\u001b[39m\u001b[32m    594\u001b[39m     input_embeds: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    595\u001b[39m ) -> Union[torch.Tensor, \u001b[38;5;28mlist\u001b[39m[torch.Tensor]]:\n\u001b[32m    596\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m        tokens (Optional[torch.Tensor]): input tensor with shape ``[b x s]``\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    651\u001b[39m \u001b[33;03m        - m_s: max seq len\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# shape: [b, s, d]\u001b[39;00m\n\u001b[32m    664\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.tok_embeddings(tokens) \u001b[38;5;28;01mif\u001b[39;00m input_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m input_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vanshaj/torchtune/torchtune/modules/transformer.py:582\u001b[39m, in \u001b[36mTransformerDecoder._validate_inputs\u001b[39m\u001b[34m(self, tokens, mask, encoder_input, encoder_mask, input_pos, input_embeds)\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    577\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mKV-caches for cross-attention/fusion layers are setup for inference mode and you seem to be using\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoder_input, causal masks must be provided! Use the `encoder_mask` arg to provide a causal mask.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m     )\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mKV-caches are setup for inference mode, input positions must be provided!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: KV-caches are setup for inference mode, input positions must be provided!"
     ]
    }
   ],
   "source": [
    "def create_causal_mask(\n",
    "    batch_size: int,\n",
    "    current_seq_len: int,  # This is (1+n) where n is number of medusa heads\n",
    "    cached_seq_len: int,   # Current KV cache length\n",
    "    max_cache_size,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype = torch.bool\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal mask for Medusa evaluation with KV cache.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of sequences in batch (typically 1 for Medusa)\n",
    "        current_seq_len: Length of new tokens being processed (1 + medusa_heads)\n",
    "        cached_seq_len: Length of tokens already in KV cache\n",
    "        device: Device to create tensor on\n",
    "        dtype: Data type for the mask\n",
    "    \n",
    "    Returns:\n",
    "        Causal mask of shape [batch_size, current_seq_len, total_seq_len]\n",
    "    \"\"\"\n",
    "    total_seq_len = cached_seq_len + current_seq_len\n",
    "    \n",
    "    # Create a lower triangular mask for the full sequence\n",
    "    mask = torch.tril(\n",
    "        torch.ones(\n",
    "            current_seq_len, \n",
    "            total_seq_len, \n",
    "            device=device, \n",
    "            dtype=dtype\n",
    "        )\n",
    "    )\n",
    "    suffix_mask_dim = max_cache_size - mask.shape[-1]\n",
    "    breakpoint()\n",
    "    mask_suffix = torch.zeros((current_seq_len , suffix_mask_dim), device = device)\n",
    "    full_mask = torch.cat((mask, mask_suffix), dim = -1) \n",
    "    # Expand to batch dimension\n",
    "    full_mask = full_mask.unsqueeze(0)  # [1, current_seq_len, total_seq_len]\n",
    "    full_mask = full_mask.expand(batch_size, current_seq_len, max_cache_size)\n",
    "    \n",
    "    return full_mask\n",
    "\n",
    "def evaluate(dataloader, model, tokens_to_generate = 10):\n",
    "    # initialize kv cache\n",
    "    for batch in dataloader:\n",
    "        model.reset_caches()\n",
    "        \n",
    "        # empty kv cache\n",
    "        input_tokens = batch['tokens'].to(device)\n",
    "        input_prompt = format_input(input_tokens)\n",
    "        breakpoint()\n",
    "        bs = input_prompt.shape[0]; curr_seq_len = input_prompt.shape[1] \n",
    "        curr_kv_len = curr_seq_len\n",
    "        causal_mask = create_causal_mask(bs, curr_seq_len, curr_kv_len, max_cache_size, device)\n",
    "        breakpoint()\n",
    "        print(causal_mask.shape)\n",
    "\n",
    "        output = model(input_prompt, mask = causal_mask) # shape: [(1+n), bs, seq, vocab_dim]\n",
    "        base_logits = output[0][:, -1] # shape: [bs, vocab_dim]\n",
    "        pred = base_logits.argmax(dim = -1) # shape: [bs, 1]\n",
    "\n",
    "        medusa_logits = torch.stack(output[1:])[:, :, -1] # shape: [n, bs, vocab_dim]\n",
    "        medusa_out = medusa_logits.argmax(dim = -1) # shape: [n, bs]\n",
    "        medusa_out = medusa_out.permute((1,0)) # shape: [bs, n]\n",
    "        tokens_generated = 1\n",
    "        preds = torch.cat((pred.unsqueeze(-1), medusa_out), dim = -1) # shape: [bs, 1+n]\n",
    "        accept_lengths = []\n",
    "        curr_kv_len = input_prompt.shape[-1]\n",
    "        pass_idx = 0\n",
    "        # breakpoint()\n",
    "        while(tokens_generated<tokens_to_generate):\n",
    "            pass_idx += 1\n",
    "            #now take all of the previous outputs and put them into the model as a batch\n",
    "            curr_seq_len = preds.shape[1] \n",
    "            causal_mask = create_causal_mask(bs, curr_seq_len, curr_kv_len, max_cache_size, device)\n",
    "            # shape: [bs, curr_seq, self.encoder_max_cache_seq_len], boolean mask with True representing queries to attend\n",
    "            \n",
    "            # All True rect mask of new_tokens x tokens_generated | upper_triangular mask of new_tokens x new_tokens + False rect mask of new_tokens x (encoder_max_cache_seq_len - (tokens_generated + new_tokens))\n",
    "            \n",
    "            pred = model(preds, mask = causal_mask) # shape: [(1+n), bs, (1+n), vocab_dim]\n",
    "\n",
    "            \n",
    "            base_logits = pred[0] # shape: [bs, (1+n), vocab_dim]\n",
    "            medusa_logits = torch.stack(pred[1:]) # shape: [n, bs, (1+n), vocab_dim]\n",
    "            base_out = base_logits.argmax(dim = -1) # shape: [bs, (1+n)]\n",
    "            medusa_out = medusa_logits.argmax(dim = -1) # shape: [n, bs, (1+n)]\n",
    "            \n",
    "            # compare base_out with preds to see which medusa_heads in the prev inference were correct:\n",
    "            mask = (base_out[:, :-1] == preds[:, 1:])\n",
    "            correct_pred_mask = mask.cumprod(dim = -1)\n",
    "            last_accepted_head = correct_pred_mask.sum()\n",
    "\n",
    "            # accept_len denotes the last head that was correct. If the last head was correct then when it is inputted back into the model, the output will also be relevant (with the base_out also being correct). Therefore the base_out is taken as an accepted token and the medusa_out is taken as the input for the next pass.\n",
    "            curr_kv_len += (last_accepted_head)\n",
    "            # reset kv cache to curr_kv_len\n",
    "            tokens_generated += (last_accepted_head+1)\n",
    "            \n",
    "\n",
    "            # what should be the input for the next pass? The last medusa pred that was correct. Take it's output as the input for the next pass.\n",
    "            accepted_head_medusa_pred = medusa_out[:, :, last_accepted_head] # shape: [n, bs]\n",
    "            accepted_head_medusa_pred = accepted_head_medusa_pred.transpose(0, 1)\n",
    "            preds = torch.cat((base_out[:, last_accepted_head: last_accepted_head + 1], accepted_head_medusa_pred), dim = -1)\n",
    "            accept_lengths.append((last_accepted_head+1).item())\n",
    "            \n",
    "            # Extract the accepted tokens for decoding\n",
    "            accepted_tokens = base_out[0, :last_accepted_head+1]  # shape: [last_accepted_head+1]\n",
    "            print(f\"Prediction {pass_idx}: \", tokenizer.decode(accepted_tokens.flatten().tolist(), skip_special_tokens=False))\n",
    "            # preds is the new input for the next pass\n",
    "            # breakpoint()\n",
    "\n",
    "\n",
    "            # new_output[0, 0] == output[0, 1]\n",
    "            # new_output[1, 0] == output[0, 2]\n",
    "            # new_output[2, 0] == output[0, 3] \n",
    "            # Then new_output[3, :] (of shape = 1 x [1 + medusa_heads] x vocab_dim) will be the batched input for the next model (which will be of shape = [1 + medusa_heads] x 1 x vocab_dim)\n",
    "            # kv cache has to be cleared at some point. Which point will that be? And what is cached inside the kv cache? Everytime inference occurs, the kv values are cached, including for the new input. The outputs kv values are not cached since they're not yet calculated. Therefore, if there are no all values tally, then the kv values for the entire batch will be cached. But they won't be cached sequentially will they? Correct, so instead of passing the input as a vector, it will have to be a matrix of [1 + medusa_heads] x [1 + medusa_heads] similar to an attention mask. We want to save the kv cache of the most recently successful row and delete all others. How does this work with a batch of inputs though?\n",
    "            # At this point the attention_mask will have\n",
    "\n",
    "        print(\"accept_lengths: \", accept_lengths)\n",
    "        return\n",
    "\n",
    "\n",
    "evaluate(dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
