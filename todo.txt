1. check chat template accepts/parses prompts in the same format bw training, inf. Verified.
2. also what about the attention mask. Abstracted in the MedusaTransformerDecoder
3. how does medusa finetuning take place? is it just next token prediction? Yes
4. What decoding strategy was used in the finetuning recipe? Is it in the loss function? Greedy, Tree Attention.



5. Reset kv cache. Also log metrics only for assistant responses?
6. How does kv caching work for a batch of inputs and if I'm batching the medusa_output, does that append to the kv cache of the actual input? So if B x [1+n] is the output dim, and I batch the inputs, what dimensionality does it have? Is it B[1+n] x 1 ? 

7. Make sure that the decoding is occuring correctly and that the kv cache is being used correctly too